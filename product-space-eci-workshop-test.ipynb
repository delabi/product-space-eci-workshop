{"cells":[{"cell_type":"markdown","metadata":{},"source":"Review session: Trade, product space and economic complexity\n============================================================\n\n"},{"cell_type":"markdown","metadata":{},"source":["September 16 2021, Matte Hartog\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Notes\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Google colab link:\n\n[https://colab.research.google.com/github/matteha/product-space-eci-workshop/blob/main/product-space-eci-workshop.ipynb](https://colab.research.google.com/github/matteha/product-space-eci-workshop/blob/main/product-space-eci-workshop.ipynb)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## To do first\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In Google Colab:\n\n1.  Turn on Table of Contents: (in browser, click on &rsquo;View&rsquo; in top, then &rsquo;Table of Contents&rsquo;)\n\n2.  Expand all sections (&rsquo;View&rsquo; > &rsquo;Expand Sections&rsquo; if not greyed out)\n\n(In Google Colab equations will show up properly, in github they don&rsquo;t work)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Outline of lab session\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Introduction to trade data\n\n-   Calculating RCAs, product co-occurences and product proximity, density / density regressions\n\n-   Product space visualization\n\n-   Calculating Economic Complexity / Product Complexity\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Trade data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Background\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The product space is, as well as its derivations / related measures such as economic complexity and the Growth&rsquo;s annual rankings of countries by economic complexity (at [https://atlas.cid.harvard.edu](https://atlas.cid.harvard.edu)), are based on trade data between countries.\n\nThe Growth Lab maintains and periodically updates a cleaned version of trade data at Harvard Dataverse:\n\n[https://dataverse.harvard.edu/dataverse/atlas](https://dataverse.harvard.edu/dataverse/atlas)\n\nThis dataset contains bilateral trade data among 235 countries and territories in thousands of different products categories (a description of the data can be found at: [http://atlas.cid.harvard.edu/downloads](http://atlas.cid.harvard.edu/downloads)).\n\nHow does the data look like? We will explore the data in Python using the &rsquo;pandas&rsquo; (most popular Python package for data analysis).\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Footnote on trade and services (ICT, tourism, etc.):\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Services and tourism are included in the Growth Lab&rsquo;s Atlas and trade data as well as of September 2018. See announcement at:\n\n[https://atlas.cid.harvard.edu/announcements/2018/services-press-release](https://atlas.cid.harvard.edu/announcements/2018/services-press-release)\n\nObtained from IMF, trade in services covers four categories of economic activities between producers and consumers across borders:\n\n-   services supplied from one country to another (e.g. call centers)\n-   consumption in other countries (e.g. international tourism)\n-   firms with branches in other countries (e.g. bank branches overseas)\n-   individuals supplying services in another country (e.g. IT consultant abroad)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Load necessary Python libraries\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# -- Global settings\n# - import python libraries necessary for this workshop\n# suppress warnings on google colab for now\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# to interact with os, e.g. to execute shell comands such as 'ls', 'pwd' etc.\nimport os\n# to do data processing\nimport pandas as pd\n# backend of pandas, working with matrices\nimport numpy as np\n# to visualize data (in pandas)\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n# to process a json file\nimport json\n# work with regex in python\nimport re\n# work with networks in python, to create product space\nimport networkx as nx\n# python tools to work with combinations of arrays\nfrom itertools import count\nfrom itertools import combinations\nfrom itertools import product\n# to run regressions\nimport statsmodels.api as sm\n# to download files\nimport urllib.request, json\n# -- set scientific notation to display numbers fully rather than exponential\npd.set_option('display.float_format', '{:.2f}'.format)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all' # Show all results of jupyter\nimport seaborn as sns\nsns.set_style('whitegrid') # Display grids on dark background\n# Enlarged pandas display - more colums and rows with greater width\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 100000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_colwidth',300)\nprint('necessary libraries loaded')"]},{"cell_type":"markdown","metadata":{},"source":["### Download trade dataset and load into memory\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Load the necessary data into pandas\n\n# In pandas terminilogy this is called a 'dataframe' (df)\nproduct_classification = 'hs' # Harmonized System 1992; alternative is 'SITC - Standard Industrial Trade Classification'\n\nN_digits = '4' # alternative is 2 or 6, the higher the more detailed product info\n\n\n# Trade data: we're using s3 storage from Amazon here because we can directly download the data into pandas in Google Colab but this is no longer maintained by the Growth Lab - rather download from Dataverse.\n\ndata_url = f\"https://intl-atlas-downloads.s3.amazonaws.com/country_{product_classification}product{N_digits}digit_year.csv.zip\"\nprint('Downloading data and loading into memory')\ndf_orig = pd.read_csv(data_url, compression=\"zip\", low_memory=False)\n\n# Fix product label strings ('hs_product_name_short_en') (some products with different product codes erronuously have the same strings - hence remove these duplicates)\n# e.g. product codes 5209 and 5211 in Zimbabwe have same product string\n# download original UN classification\nwith urllib.request.urlopen(\"https://comtrade.un.org/data/cache/classificationH0.json\") as url:\n    hs1992_json = json.loads(url.read())\ndft = pd.DataFrame.from_dict(hs1992_json['results'])[['text']]\ndft['hs_product_code'] = dft['text'].str.split('-').str[0].str.strip()\ndft['hs_product_name_short_en'] = dft['text'].str.split('-',1).str[1].str.strip()\ndft['N_dig'] = dft['hs_product_code'].str.len()\ndft2 = dft[dft['N_dig']==int(N_digits)].copy()\ndf_orig = pd.merge(df_orig,dft2[['hs_product_code','hs_product_name_short_en']],how='left',on=f'hs_product_code') # unmerged are services (obtained from IMF)\n# replace product name now with downloaded strings (if not missing in either)\ndf_orig['hs_product_name_short_en_new'] = df_orig['hs_product_name_short_en_x']\ndf_orig.loc[ df_orig['hs_product_name_short_en_y'].notnull(),'hs_product_name_short_en_new'] = df_orig['hs_product_name_short_en_y']\ndf_orig.drop(['hs_product_name_short_en_x'],axis=1,inplace=True,errors='ignore')\ndf_orig.drop(['hs_product_name_short_en_y'],axis=1,inplace=True,errors='ignore')\ndf_orig.rename(columns={f'hs_product_name_short_en_new':f'hs_product_name_short_en'}, inplace=True)\n\n# Cross check that each row is a unique year-location-product entry\ndf_orig['count'] = 1\ndf_orig['sum'] = df_orig.groupby(['year','location_name_short_en','hs_product_name_short_en'])['count'].transform('sum')\nif df_orig['sum'].max() != 1:\n    print(f'duplicates found, stopping')\n    stop\n\n# rename variable names for convenience\ndf_orig.rename(columns={f'location_name_short_en':f'country_name'}, inplace=True)\ndf_orig.rename(columns={f'location_code':f'country_code'}, inplace=True)\ndf_orig.rename(columns={f'hs_product_code':f'product_code'}, inplace=True)\ndf_orig.rename(columns={f'hs_product_name_short_en':f'product_name'}, inplace=True)\n\n# Keep only relevant columns\ndf_orig = df_orig[['year',\n         'country_code',\n         'country_name',\n         'product_code',\n         'product_name',\n         'export_value']]\n\nprint('trade dataset ready')"]},{"cell_type":"markdown","metadata":{},"source":["### Exploring the trade data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Structure of dataset\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# show 5 random rows\ndf_orig.sample(n=5)"]},{"cell_type":"markdown","metadata":{},"source":["#### What years are in the data?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df_orig['year'].unique()"]},{"cell_type":"markdown","metadata":{},"source":["#### How many products are in the data?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df_orig['product_name'].nunique()"]},{"cell_type":"markdown","metadata":{},"source":["#### Finding specific countries / products based on partial string matching\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"# [goto error]\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_10002/3130682011.py in <module>\n      1 STRING = 'Netherland'\n----> 2 df_orig[df_orig['country_name'].str.contains(STRING)][['country_name']].drop_duplicates()\n      3 \n      4 # Can also include regex expressions here, e.g. to ignore lower/uppercase ('wine' vs 'Wine')\n      5 STRING = 'wine'\n\nNameError: name 'df_orig' is not defined"}],"source":["STRING = 'Netherland'\ndf_orig[df_orig['country_name'].str.contains(STRING)][['country_name']].drop_duplicates()\n\n# Can also include regex expressions here, e.g. to ignore lower/uppercase ('wine' vs 'Wine')\nSTRING = 'wine'\ndf_orig[df_orig['product_name'].str.contains(STRING,flags=re.IGNORECASE, regex=True)][['product_name']].drop_duplicates()"]},{"cell_type":"markdown","metadata":{},"source":["#### Example: What were the major export products of the USA in 2012?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# create a 'dataframe' called 'df2' with only exports from USA in 2012\ndf2 = df_orig[ (df_orig['country_code']=='USA') & (df_orig['year'] == 2012) ].copy()\n# create another dataframe 'df3' that contains the sum of exports per product\ndf3 = df2.groupby(['product_code','product_name'],as_index=False)['export_value'].sum()\n# sort\ndf3.sort_values(by=['export_value'],ascending=False,inplace=True)\n# show first 10 rows\ndf3[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### Example: How did exports of Cars evolve over time in the USA?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["From about 10 billion USD up to almost $60 billion USD.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df2 = df_orig[ (df_orig['country_code']=='USA')].copy()\n#df3 = df2[df2['product_name']=='Cars']\ndf3 = df2[df2['product_code']=='8703']\ndf3.plot(x='year', y='export_value')"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
