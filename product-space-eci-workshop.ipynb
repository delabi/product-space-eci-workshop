{"cells":[{"cell_type":"markdown","metadata":{},"source":"Trade and patents in Python: RCAs, proximities, product space and economic complexity\n=====================================================================================\n\n"},{"cell_type":"markdown","metadata":{},"source":["September 16 2021, Matte Hartog\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Notes\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Google colab link:\n\n[https://colab.research.google.com/github/matteha/product-space-eci-workshop/blob/main/product-space-eci-workshop.ipynb](https://colab.research.google.com/github/matteha/product-space-eci-workshop/blob/main/product-space-eci-workshop.ipynb)\n\nTo run code in Google Colab, you need a Google account.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## To do first\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In Google Colab:\n\n1.  Turn on Table of Contents: (in browser, click on &rsquo;View&rsquo; in top, then &rsquo;Table of Contents&rsquo;)\n\n2.  Expand all sections (&rsquo;View&rsquo; > &rsquo;Expand Sections&rsquo; if not greyed out)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Outline of lab session\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Introduction to trade data\n\n-   Calculating RCAs, product co-occurences and product proximities\n\n-   Product space visualization\n\n-   Calculating Economic Complexity / Product Complexity\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Trade data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Background\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The product space is, as well as its derivations / related measures such as economic complexity and the Growth&rsquo;s annual rankings of countries by economic complexity (at [https://atlas.cid.harvard.edu](https://atlas.cid.harvard.edu)), are based on trade data between countries.\n\nThe Growth Lab maintains and periodically updates a cleaned version of trade data at Harvard Dataverse:\n\n[https://dataverse.harvard.edu/dataverse/atlas](https://dataverse.harvard.edu/dataverse/atlas)\n\nThis dataset contains bilateral trade data among 235 countries and territories in thousands of different products categories (a description of the data can be found at: [http://atlas.cid.harvard.edu/downloads](http://atlas.cid.harvard.edu/downloads)).\n\nHow does the data look like? We will explore the data in Python using the &rsquo;pandas&rsquo; (most popular Python package for data analysis).\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Footnote on trade and services (ICT, tourism, etc.):\n\n"]},{"cell_type":"markdown","metadata":{},"source":["-   Services and tourism are included in the Growth Lab&rsquo;s Atlas and trade data as well as of September 2018. See announcement at:\n\n[https://atlas.cid.harvard.edu/announcements/2018/services-press-release](https://atlas.cid.harvard.edu/announcements/2018/services-press-release)\n\nObtained from IMF, trade in services covers four categories of economic activities between producers and consumers across borders:\n\n-   services supplied from one country to another (e.g. call centers)\n-   consumption in other countries (e.g. international tourism)\n-   firms with branches in other countries (e.g. bank branches overseas)\n-   individuals supplying services in another country (e.g. IT consultant abroad)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Load necessary Python libraries\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# -- Global settings\n# - import python libraries necessary for this workshop\n# suppress warnings on google colab for now\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# to interact with os, e.g. to execute shell comands such as 'ls', 'pwd' etc.\nimport os\n# to do data processing\nimport pandas as pd\n# backend of pandas, working with matrices\nimport numpy as np\n# to visualize data (in pandas)\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nfrom matplotlib import cm\n# to process a json file\nimport json\n# work with regex in python\nimport re\n# work with networks in python, to create product space\nimport networkx as nx\n# python tools to work with combinations of arrays\nfrom itertools import count\nfrom itertools import combinations\nfrom itertools import product\n# to run regressions\nimport statsmodels.api as sm\n# to download files\nimport urllib.request, json\n# to plot data on maps\n# pip install is needed on Google Colab\n!pip3 install geopandas\nimport geopandas\n# -- set scientific notation to display numbers fully rather than exponential\npd.set_option('display.float_format', '{:.2f}'.format)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all' # Show all results of jupyter\nimport seaborn as sns\nsns.set_style('whitegrid') # Display grids on dark background\n# Enlarged pandas display - more colums and rows with greater width\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 100000)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_colwidth',300)\nprint('necessary libraries loaded')"]},{"cell_type":"markdown","metadata":{},"source":["### Download trade dataset and load into memory\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Load the necessary data into pandas\n\n# In pandas terminilogy this is called a 'dataframe' (df)\nproduct_classification = 'hs' # Harmonized System 1992; alternative is 'SITC - Standard Industrial Trade Classification'\n\nN_digits = '4' # alternative is 2 or 6, the higher the more detailed product info\n\n# Trade data: we're using s3 storage from Amazon here because we can directly download the data into pandas in Google Colab but this is no longer maintained by the Growth Lab - rather download from Dataverse.\n\ndata_url = f\"https://intl-atlas-downloads.s3.amazonaws.com/country_{product_classification}product{N_digits}digit_year.csv.zip\"\nprint('Downloading data and loading into memory')\ndf_orig = pd.read_csv(data_url, compression=\"zip\", low_memory=False)\n\n# Fix product label strings ('hs_product_name_short_en') (some products with different product codes erronuously have the same strings - hence remove these duplicates)\n# e.g. product codes 5209 and 5211 in Zimbabwe have same product string\n# download original UN classification\nwith urllib.request.urlopen(\"https://comtrade.un.org/data/cache/classificationH0.json\") as url:\n    hs1992_json = json.loads(url.read())\ndft = pd.DataFrame.from_dict(hs1992_json['results'])[['text']]\ndft['hs_product_code'] = dft['text'].str.split('-').str[0].str.strip()\ndft['hs_product_name_short_en'] = dft['text'].str.split('-',1).str[1].str.strip()\ndft['N_dig'] = dft['hs_product_code'].str.len()\ndft2 = dft[dft['N_dig']==int(N_digits)].copy()\ndf_orig = pd.merge(df_orig,dft2[['hs_product_code','hs_product_name_short_en']],how='left',on=f'hs_product_code') # unmerged are services (obtained from IMF)\n# replace product name now with downloaded strings (if not missing in either)\ndf_orig['hs_product_name_short_en_new'] = df_orig['hs_product_name_short_en_x']\ndf_orig.loc[ df_orig['hs_product_name_short_en_y'].notnull(),'hs_product_name_short_en_new'] = df_orig['hs_product_name_short_en_y']\ndf_orig.drop(['hs_product_name_short_en_x'],axis=1,inplace=True,errors='ignore')\ndf_orig.drop(['hs_product_name_short_en_y'],axis=1,inplace=True,errors='ignore')\ndf_orig.rename(columns={f'hs_product_name_short_en_new':f'hs_product_name_short_en'}, inplace=True)\n\n# Cross check that each row is a unique year-location-product entry\ndf_orig['count'] = 1\ndf_orig['sum'] = df_orig.groupby(['year','location_name_short_en','hs_product_name_short_en'])['count'].transform('sum')\nif df_orig['sum'].max() != 1:\n    print(f'duplicates found, stopping')\n    stop\n\n# rename variable names for convenience\ndf_orig.rename(columns={f'location_name_short_en':f'country_name'}, inplace=True)\ndf_orig.rename(columns={f'location_code':f'country_code'}, inplace=True)\ndf_orig.rename(columns={f'hs_product_code':f'product_code'}, inplace=True)\ndf_orig.rename(columns={f'hs_product_name_short_en':f'product_name'}, inplace=True)\n\n# Keep only relevant columns\ndf_orig = df_orig[['year',\n         'country_code',\n         'country_name',\n         'product_code',\n         'product_name',\n         'export_value']]\n\nprint('trade dataset \\'df_orig\\' ready')"]},{"cell_type":"markdown","metadata":{},"source":["### Exploring the trade data\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Structure of dataset\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# show 5 random rows\ndf_orig.sample(n=5)"]},{"cell_type":"markdown","metadata":{},"source":["#### What years are in the data?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df_orig['year'].unique()"]},{"cell_type":"markdown","metadata":{},"source":["#### How many products are in the data?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df_orig['product_name'].nunique()"]},{"cell_type":"markdown","metadata":{},"source":["#### Finding specific countries / products based on partial string matching\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"# [goto error]\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipykernel_10002/3130682011.py in <module>\n      1 STRING = 'Netherland'\n----> 2 df_orig[df_orig['country_name'].str.contains(STRING)][['country_name']].drop_duplicates()\n      3 \n      4 # Can also include regex expressions here, e.g. to ignore lower/uppercase ('wine' vs 'Wine')\n      5 STRING = 'wine'\n\nNameError: name 'df_orig' is not defined"}],"source":["STRING = 'Netherland'\ndf_orig[df_orig['country_name'].str.contains(STRING)][['country_name']].drop_duplicates()\n\n# Can also include regex expressions here, e.g. to ignore lower/uppercase ('wine' vs 'Wine')\nSTRING = 'wine'\ndf_orig[df_orig['product_name'].str.contains(STRING,flags=re.IGNORECASE, regex=True)][['product_name']].drop_duplicates()"]},{"cell_type":"markdown","metadata":{},"source":["#### Example: What were the major export products of the USA in 2012?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# create a 'dataframe' called 'df2' with only exports from USA in 2012\ndf2 = df_orig[ (df_orig['country_code']=='USA') & (df_orig['year'] == 2012) ].copy()\n# create another dataframe 'df3' that contains the sum of exports per product\ndf3 = df2.groupby(['product_code','product_name'],as_index=False)['export_value'].sum()\n# sort\ndf3.sort_values(by=['export_value'],ascending=False,inplace=True)\n# show first 10 rows\ndf3[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### Example: How did exports of Cars evolve over time in the USA?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["From about 10 billion USD up to almost $60 billion USD.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df2 = df_orig[ (df_orig['country_code']=='USA')].copy()\n#df3 = df2[df2['product_name']=='Cars']\ndf3 = df2[df2['product_code']=='8703']\ndf3.plot(x='year', y='export_value')"]},{"cell_type":"markdown","metadata":{},"source":["## Revealed comparative advantage (RCA)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["What products are countries specialized in? For that, following Hidalgo et al. (2007), we calculate the Revealed Comparative Advantage (RCA) of each country-product pair: how much a country &rsquo;over-exports&rsquo; a product in comparison to all other countries.\n\nTechnically this is the Balassa index of comparative advantage, calculated as follows for product $p$ and country $c$ at time $t$:\n\n\\begin{equation} \\label{e_RCA}\n{RCA}_{cpt}=\\frac{X_{cpt}/X_{ct}}{X_{pt}/X_{t}}\n\\tag{1}\n\\end{equation}\n\nwhere $X_{cpt}$ represents the total value of country $c$’s exports of product $p$ at time $t$ across all importers. An omitted subscript indicates a summation over the omitted dimension, e.g.: $X_{t}=\\sum \\limits_{c,p,t} X_{cpt}$.\n\nA product-country pair with $RCA>1$ means that the product is over-represented in the country&rsquo;s export basket.\n\nWe use the original trade dataset (&rsquo;df<sub>orig</sub>&rsquo;) that is loaded into memory:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def calc_rca(data,country_col,product_col,time_col,value_col):\n    \"\"\"\n    Calculates Revealed Comparative Advantage (RCA) of country-product-time combinations\n\n    Returns:\n        pandas dataframe with RCAs\n    \"\"\"\n\n    # Aggregate to country-product-time dataframe\n    print('creating all country-product-time combinations')\n    # - add all possible products for each country with export value 0\n    # - else matrices later on will have missing values in them, complicating calculations\n    df_all = pd.DataFrame(list(product(data[time_col].unique(), data[country_col].unique(),data[product_col].unique())))\n    df_all.columns=[time_col,country_col,product_col]\n    print('merging data in')\n    df_all = pd.merge(df_all,data[[time_col,country_col,product_col,value_col]],how='left',on=[time_col,country_col,product_col])\n    df_all.loc[df_all[value_col].isnull(),value_col] = 0\n\n    # Calculate the properties\n    print('calculating properties')\n    df_all['Xcpt'] = df_all[value_col]\n    df_all['Xct'] = df_all.groupby([country_col, time_col])[value_col].transform(sum)\n    df_all['Xpt'] = df_all.groupby([product_col, time_col])[value_col].transform(sum)\n    df_all['Xt'] = df_all.groupby([time_col])[value_col].transform('sum')\n    df_all['RCAcpt'] = (df_all['Xcpt']/df_all['Xct'])/(df_all['Xpt']/df_all['Xt'])\n    df_all.drop(['Xcpt','Xct','Xpt','Xt'],axis=1,inplace=True,errors='ignore')\n\n    return df_all\n\ndf_rca = calc_rca(data=df_orig,country_col='country_name',product_col='product_name',time_col='year',value_col='export_value')\n\nprint('rca dataframe ready')\n\n# show results\ndf_rca[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["### Example: What products are The Netherlands and Saudi Arabia specialized in, in 2000?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["(Note that different commands are chained together here; can also be ran separately)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# The Netherlands\nprint(\"\\n The Netherlands: \\n\")\n\ndf_rca[ (df_rca['year']==2000) & (df_rca['country_name']=='Netherlands')].sort_values(by=['RCAcpt'],ascending=False)[['product_name','RCAcpt','year']][0:5]\n\nprint(\"\\n Saudi Arabia:\\n\")\n\n# Saudi Arabia\ndf_rca[ (df_rca['year']==2000) & (df_rca['country_name']=='Saudi Arabia')].sort_values(by=['RCAcpt'],ascending=False)[['product_name','RCAcpt','year']][0:5]"]},{"cell_type":"markdown","metadata":{},"source":["## Product proximity (based on co-occurences)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Calculating product co-occurences and proximities\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Knowing which countries are specialized in which products, the next step analyzes the extent to which two products are over-represented ($RCA>1$) in the same countries.\n\nAs noted in the lecture, the main insight supporting this inference is that countries will produce combinations of products that require similar capabilities.\n\nHence we infer capabilities from trade patterns, because the capabilities of a country is a priori hard to determine and capabilities themselves are hard to observe.\n\nHence, **the degree to which two products cooccur in the export baskets of the same countries provides an indication of how similar the capability requirements of the two products are**.\n\nWe will calculate the co-occurence matrix of products below.\n\nFirst, a product is &rsquo;present&rsquo; in a country if the country exports the product with $RCA>1$:\n\n\\begin{equation} \\label{e_presence}\nM_{cp}=\\begin{cases}\n    1 & \\text{if ${RCA}_{cp}>1$}; \\\\\n    0 & \\text{elsewhere.}\n    \\end{cases}\n\\tag{2}\n\\end{equation}\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df_rca['Mcp'] = 0\ndf_rca.loc[df_rca['RCAcpt']>1,'Mcp'] = 1"]},{"cell_type":"markdown","metadata":{},"source":["Next, we calculate how often two products are present in the same countries, using the Mcp threshold:\n\n\\begin{equation} \\label{e_cooc}\nC_{pp'}=\\sum \\limits_{c} M_{cp} M_{cp'}\n\\tag{3}\n\\end{equation}\n\nTo get an accurate value of product proximity, we need to correct these numbers for the extent to which products are present in general in trade flows between countries. To do so, Hidalgo et al. (2007) calculate product proximity as follows, defining it as the minimum of two conditional probabilities:\n\n\\begin{equation}\nC_{ppt'}  = \\min \\left( \\frac{C_{pp'}}{C_{p}},\\frac{C_{pp'}}{C_{p'}} \\right)\n\\tag{4}\n\\end{equation}\n\nThe minimum here is used to elimate a &rsquo;false positive&rsquo;.\n\nHence we correct for how prevalent specialization in product $i$ and product $j$ is across countries (i.e. the &rsquo;ubiquity&rsquo; of the products).\n\nWe will use the first year of data in the dataset, **1995,** below.\n\nNote that to reduce yearly votality, Hidalgo et al. (2007) aggregate the trade data across multiple years (1998-2000) when calculating RCAs and product proximities for the product space. (However, when comparing the product space across years, they do use individual years).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def calc_cppt(data,country_col,product_col):\n    \"\"\"\n    Calculates product co-occurences in countries\n\n    Returns:\n        pandas dataframe with co-occurence value for each product pair\n    \"\"\"\n\n    # Create combinations within country_col (i.e. countries) of entities (i.e. products)\n    dft = (data.groupby(country_col)[product_col].apply(lambda x: pd.DataFrame(list(combinations(x,2))))\n            .reset_index(level=1, drop=True)\n            .reset_index())\n    dft.rename(columns={0:f'{product_col}_1'}, inplace=True)\n    dft.rename(columns={1:f'{product_col}_2'}, inplace=True)\n\n    # Create second half of matrix (assymmetrical):\n    # -- {product_col} 1 X {product_col} 2 == {product_col} 2 X {product_col} 1\n    dft2 = dft.copy()\n    dft2.rename(columns={f'{product_col}_1':f'{product_col}_2t'}, inplace=True)\n    dft2.rename(columns={f'{product_col}_2':f'{product_col}_1'}, inplace=True)\n    dft2.rename(columns={f'{product_col}_2t':f'{product_col}_2'}, inplace=True)\n    # -- add second half\n    dft3 = pd.concat([dft,dft2],axis=0,sort=False)\n\n    # Drop diagonal if present\n    dft3 = dft3[ dft3[f'{product_col}_1'] != dft3[f'{product_col}_2'] ]\n\n    # Now calculate N of times that {product_col}s occur together\n    dft3['count'] = 1\n    dft3 = dft3.groupby([f'{product_col}_1',f'{product_col}_2'],as_index=False)['count'].sum()\n    dft3.rename(columns={f'count':f'Cpp'}, inplace=True)\n\n    # Calculate ubiquity\n    df_ub = data.groupby(product_col,as_index=False)['Mcp'].sum()\n    # Merge ubiqity into cpp matrix\n    df_ub.rename(columns={f'{product_col}':f'{product_col}_1'}, inplace=True)\n    dft3 = pd.merge(dft3,df_ub,how='left',on=f'{product_col}_1')\n    df_ub.rename(columns={f'{product_col}_1':f'{product_col}_2'}, inplace=True)\n    dft3 = pd.merge(dft3,df_ub,how='left',on=f'{product_col}_2')\n\n    # Take minimum of conditional probabilities\n    dft3['kpi'] = dft3['Cpp']/dft3['Mcp_x']\n    dft3['kpj'] = dft3['Cpp']/dft3['Mcp_y']\n    dft3['phi'] = dft3['kpi']\n    dft3.loc[dft3['kpj']<dft3['kpi'],'phi'] = dft3['kpj']\n\n    return dft3\n\n# Keep only year 1995\ndft = df_rca[df_rca['year']==1995].copy()\n\n# Keep only country-product combinations where Mcp == 1 (thus RCAcp > 1)\ndft = dft[dft['Mcp']==1]\n\n# Calculate cpp\ndf_cppt = calc_cppt(dft,country_col='country_name',product_col='product_name')\n\nprint('cpp product co-occurences dataframe ready')"]},{"cell_type":"markdown","metadata":{},"source":["#### Products that co-occur most often\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# -- show products that co-occur most often\ndf_cppt.sort_values(by=['Cpp'],ascending=False)[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### Most proximate products\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"#+begin_example\n                                                                                                                                                                                product_name_1                                                                                                                                                                         product_name_2  Cpp  Mcp_x  Mcp_y  kpi  kpj  phi\n690963              Machine-tools; (including machines for nailing, stapling, glueing or otherwise assembling) for working wood, cork, bone, hard plastics or rubber or similar hard materials                                                                                  Machines; calendering or other rolling machines, for other than metal or glass and cylinders therefor    9     10     10 0.90 0.90 0.90\n723013                                                                                   Machines; calendering or other rolling machines, for other than metal or glass and cylinders therefor             Machine-tools; (including machines for nailing, stapling, glueing or otherwise assembling) for working wood, cork, bone, hard plastics or rubber or similar hard materials    9     10     10 0.90 0.90 0.90\n1149849                             Suits, ensembles, jackets, blazers, trousers, bib and brace overalls, breeches and shorts (other than swimwear); men's or boys' (not knitted or crocheted)  Suits, ensembles, jackets, dresses, skirts, divided skirts, trousers, bib and brace overalls, breeches and shorts (other than swimwear); women's or girls' (not knitted or crocheted)   59     68     70 0.87 0.84 0.84\n1153522  Suits, ensembles, jackets, dresses, skirts, divided skirts, trousers, bib and brace overalls, breeches and shorts (other than swimwear); women's or girls' (not knitted or crocheted)                             Suits, ensembles, jackets, blazers, trousers, bib and brace overalls, breeches and shorts (other than swimwear); men's or boys' (not knitted or crocheted)   59     70     68 0.84 0.87 0.84\n324476                                                                                                                                                          Cork; articles of natural cork                                                                                                   Natural cork, raw or simply prepared; waste cork; crushed, granulated or ground cork    5      6      6 0.83 0.83 0.83\n808540                                                                                                    Natural cork, raw or simply prepared; waste cork; crushed, granulated or ground cork                                                                                                                                                         Cork; articles of natural cork    5      6      6 0.83 0.83 0.83\n1339374                                                                                                                                               Video recording or reproducing apparatus                                                                                             Sound or video recording apparatus; parts thereof of apparatus of heading no. 8519 to 8521   10     12     12 0.83 0.83 0.83\n137061                                                                                                              Blouses, shirts and shirt-blouses; women's or girls', knitted or crocheted              Suits, ensembles, jackets, dresses, skirts, divided skirts, trousers, bib and brace overalls, breeches and shorts (not swimwear), women's or girls', knitted or crocheted   50     54     60 0.93 0.83 0.83\n1115899                                                                                             Sound or video recording apparatus; parts thereof of apparatus of heading no. 8519 to 8521                                                                                                                                               Video recording or reproducing apparatus   10     12     12 0.83 0.83 0.83\n1151434              Suits, ensembles, jackets, dresses, skirts, divided skirts, trousers, bib and brace overalls, breeches and shorts (not swimwear), women's or girls', knitted or crocheted                                                                                                             Blouses, shirts and shirt-blouses; women's or girls', knitted or crocheted   50     60     54 0.83 0.93 0.83\n#+end_example"}],"source":["df_cppt.sort_values(by=['phi'],ascending=False)[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["## Patents: RCAs and technology proximities\n\n"]},{"cell_type":"markdown","metadata":{},"source":["At the Growth Lab we have access to the Patstat database as well as all patents from Google Bigquery (both are on the RC Cannon cluster).\n\nPatstat includes all patents from ~ 1903 onwards.\n\nBelow is an outline of what is available in Patstat:\n\n![img](https://www.dropbox.com/s/zqgv7fi61c2ip2f/patstat.png?dl=1)\n\nBelow I use an aggregated file created from the Patstat database, containing:\n\n-   Year\n-   Country\n-   Technology class\n-   Count (N of patents)\n\nwhich I put on Dropbox temporarily so we can load it in directly.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"loading file\nloaded\n#+begin_example\n         index  year country_iso_code   country_name                         tech  count\n108233  111398  2014              TUN        Tunisia      Audio-visual technology      3\n99129   102294  2011                                              Pharmaceuticals      1\n102226  105391  2012              SWE         Sweden             Furniture, games    137\n17452    20617  1947              LIE  Liechtenstein                  Measurement      2\n75559    78724  2002              BHS        Bahamas  Surface technology, coating     22\n45471    48636  1985              MYS       Malaysia       Other special machines      2\n27415    30580  1967              BEL        Belgium                  Measurement      6\n75604    78769  2002              CAN         Canada                Machine tools    364\n96976   100141  2010              THA       Thailand               Semiconductors     14\n59260    62425  1994              LVA         Latvia               Semiconductors     10\n#+end_example"}],"source":["# Load STATA file into pandas, directly from URL\nprint('loading file')\ndfp = pd.read_stata('https://www.dropbox.com/s/nwox3dznoupzm0q/patstat_year_country_tech_inventor_locations.dta?dl=1')\nprint('loaded')\n\n# show 10 random rows from the dataset\ndfp.sample(n=10)"]},{"cell_type":"markdown","metadata":{},"source":["### RCAs\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["dfp_rca = calc_rca(data=dfp,\n                   country_col='country_name',\n                   product_col='tech',\n                   time_col='year',\n                   value_col='count')\n\n# What were Japan and Germany specialized in, in 1960 and 2010?\nfor country in ['Japan','Germany']:\n    for year in [1960, 2010]:\n        print(f\"---------\")\n        print(f\"\\n {country} in {year} \\n\")\n        dft = dfp_rca[dfp_rca['country_name']==country].copy()\n        dft = dft[dft['year']==year]\n        # --\n        dft.sort_values(by=['RCAcpt'],ascending=False)[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["### Technology proximities\n\n"]},{"cell_type":"markdown","metadata":{},"source":["What technology classes are most proximate (in 2000-2010)?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Define Mcp\ndfp_rca['Mcp'] = 0\ndfp_rca.loc[dfp_rca['RCAcpt']>1,'Mcp'] = 1\n\n# Keep only years 2000-2010\ndft = dfp_rca[ (dfp_rca['year']>=1990) & (dfp_rca['year']<=2020)].copy()\n\n# Keep only country-product combinations where Mcp == 1 (thus RCAcp > 1)\ndft = dft[dft['Mcp']==1]\n\n# Calculate cppt\ndfp_cppt = calc_cppt(dft,country_col='country_name',product_col='tech')\nprint('cpp patent co-occurences dataframe ready')"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Show most proximate technologies\ndfp_cppt.sort_values(by=['phi'],ascending=False)[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["Density regressions to predict technological diversification: Principe of relatedness.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Product space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Overview\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We now have a measure of similarity between products (and patents), which is the core of the product space.\n\n[https://atlas.cid.harvard.edu/explore/network?country=114&year=2018&productClass=HS&product=undefined&startYear=undefined&target=Product&partner=undefined](https://atlas.cid.harvard.edu/explore/network?country=114&year=2018&productClass=HS&product=undefined&startYear=undefined&target=Product&partner=undefined)\n\n![img](/home/linux/Dropbox/proj/org_zhtml_projects/product-space-eci-workshop/imgs/product_space_atlas_website.png)\n\n![img](https://www.dropbox.com/s/izag1xf28yldanf/product_space_atlas_website.png?dl=1)\n\nBelow we will explore the product space using Python. You can then directly manipulate the product space and visualize selectively if not possible in the Atlas interface (e.g. only products exported to certain countries).\n\nThe Github repo for this is available at [https://github.com/matteha/py-productspace](https://github.com/matteha/py-productspace).\n\nWhat we need is information on:\n\n-   Edges (ties) between nodes\n    \n    Ties between nodes represent the product proximity calculated above. Each product pair has a proximity value, but visualizing all ties, however, would result in a major &ldquo;hairball&rdquo;.\n    \n    To determine which of the ties to visualize in the product space, a &rsquo;maximum spanning tree algorithm&rsquo; is used (to make sure all nodes are connected directly or indirectly) in conjunction with a certain proximity threshold (0.55 minimum conditional probability). The details can be found in the Supplementary Material of Hidalgo et al. (2007) at [https://science.sciencemag.org/content/suppl/2007/07/26/317.5837.482.DC1](https://science.sciencemag.org/content/suppl/2007/07/26/317.5837.482.DC1).\n\n-   Position of nodes\n    -   Each node is a product\n    \n    -   To position them in the product space, Hidalgo et al. (2007) used a spring embedding algorithm (which positions the nodes in such a way that there are as few crossing ties as possible, using physical simulations with force-directed algorithms), followed by hand-crafting the outcome to further visually separate distinct &rsquo;clusters&rsquo; of products.\n        \n        We will use this fixed layout for now (James and Yang will deal with different ways to visualize multi-dimensional data in 2D/3D, e.g. with machine learning, UMAP).\n        \n        The data on the position of nodes (x, y coordinates) is available in the Atlas data repository at:\n        [https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/FCDZBN/QSEETD&version=1.1](https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/FCDZBN/QSEETD&version=1.1)\n        \n        We can directly load it into Python using the link below (temporarily for this session, when using Harvard&rsquo;s dataverse you&rsquo;d need to sign a short User Agreement form so you can&rsquo;t load data directly from a URL):\n        \n        [https://www.dropbox.com/s/r601tjoulq1denf/network_hs92_4digit.json?dl=1](https://www.dropbox.com/s/r601tjoulq1denf/network_hs92_4digit.json?dl=1)\n\n-   Size of nodes\n    \n    The size in the product space represents the total $ in world trade, but one can also use other attributes of nodes (e.g. if nodes are industries, the size could be total employment).\n\n-   Color of nodes\n    \n    In the product space the node color represents major product groups (e.g. Agriculture, Chemicals) following the Leamer classification. The node coloring data is available in the Atlas data repository at:\n    [https://dataverse.harvard.edu/dataverse/atlas?q=&types=files&sort=dateSort&order=desc&page=1](https://dataverse.harvard.edu/dataverse/atlas?q=&types=files&sort=dateSort&order=desc&page=1)\n    \n    We can directly load it into Python using the link below (again, temporary for this session):\n    \n    [https://www.dropbox.com/s/rlm8hu4pq0nkg63/hs4_hex_colors_intl_atlas.csv?dl=1](https://www.dropbox.com/s/rlm8hu4pq0nkg63/hs4_hex_colors_intl_atlas.csv?dl=1)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Product space in Python\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Function to create product space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The function below creates the product space. It uses the &rsquo;networkx&rsquo; package.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def create_product_space(df_plot_dataframe=None,\n                         df_plot_node_col=None,\n                         df_node_size_col=None,\n                         show_legend = 0):\n\n    # Copy dataframe so original won't be overwritten\n    df_plot =  df_plot_dataframe.copy()\n\n    NORMALIZE_NODE_SIZE = 1\n    if NORMALIZE_NODE_SIZE == 1:\n\n        \"\"\"\n        The distribution of export values is highly skewed, which makes it hard to visualize\n        them properly (certain products will overshadow the rest of the network).\n\n\n        We create a new size column below in which we normalize the export values.\n        \"\"\"\n\n        ### Normalize node size (0.1 to 1)\n\n        def normalize_col(dft,col,minsize=0.1,maxsize=1):\n            \"\"\"\n            Normalizes column values with largest and smallest values capped at min at max\n            For use in networkx\n\n            returns pandas column\n            \"\"\"\n\n            alpha = maxsize-minsize\n            Xl = dft[dft[col]>0][col].quantile(0.10)\n            Xh = dft[dft[col]>0][col].quantile(0.95)\n            dft['node_size'] = 0\n            dft.loc[ dft[col]>=Xh,'node_size'] = maxsize\n            dft.loc[ (dft[col]<=Xl) & (dft[col]!=0),'node_size'] = minsize\n            dft.loc[ (dft[col]<Xh) & (dft[col]>Xl),'node_size'] = ((alpha*(dft[col]-Xl))/(Xh-Xl))+(1-alpha)\n            dft.loc[ (dft[col]<Xh) & (dft[col]>0),'node_size'] = ((alpha*(dft[col]-Xl))/(Xh-Xl))+(1-alpha)\n\n            return dft['node_size']\n\n        df_plot['node_size'] = normalize_col(df_plot,df_node_size_col,minsize=0.1,maxsize=1)\n\n    ADD_COLORS_ATLAS = 1\n    if ADD_COLORS_ATLAS == 1:\n\n        # First add product codes from original file (full strings were used for illustrative purposes above but we need the actual codes to merge data from other sources, e.g. node colors)\n        df_plot = pd.merge(df_plot,df_orig[['product_name','product_code']].drop_duplicates(),how='left',on='product_name')\n        dft = pd.read_csv('https://www.dropbox.com/s/rlm8hu4pq0nkg63/hs4_hex_colors_intl_atlas.csv?dl=1')\n\n        # Transform product_code into int (accounts for missing in pandas, if necessary)\n        # keep only numeric product_codes (this drops 'unspecified' as well as services for now;\n        # - as the latter needs a separate color classification)\n        df_plot = df_plot[df_plot['product_code'].astype(str).str.isnumeric()]\n        # -- also drop 9999 product code; unknown\n        df_plot = df_plot[df_plot['product_code'].astype(str)!='9999']\n        # -- to allow merge, rename and transform both variables into int\n        dft['hs4'] = dft['hs4'].astype(int)\n        df_plot['product_code'] = df_plot['product_code'].astype(int)\n        if 'color' in df_plot.columns:\n            df_plot.drop(['color'],axis=1,inplace=True,errors='ignore')\n        df_plot = pd.merge(df_plot,dft[['hs4','color']],how='left',left_on='product_code',right_on='hs4')\n        # drop column merged from dft\n        df_plot.drop(['hs4'],axis=1,inplace=True,errors='ignore')\n\n        CREATE_LEGEND = 1\n        if CREATE_LEGEND == 1:\n            # Atlas classification products\n            df_temp = pd.read_csv('https://raw.githubusercontent.com/cid-harvard/classifications/master/product/HS/IntlAtlas/out/hs92_atlas.csv')\n            df_temp.rename(columns={'Unnamed: 0': 'internal_code'}, inplace=True)\n            # -- keep sections\n            df_temp1 = df_temp[df_temp['level']=='section'].copy()\n            # -- keep 4 digit\n            df_temp2 = df_temp[df_temp['level']=='2digit'].copy()\n            # -- keep 4 digit\n            df_temp3 = df_temp[df_temp['level']=='4digit'].copy()\n            # -- merge parent id of parent id of 4 digits (= 2digit)\n            # ---- remake to float\n            df_temp3['parent_id'] = df_temp3['parent_id'].astype(object)\n            df_temp2['internal_code'] = df_temp2['internal_code'].astype(object)\n            df_temp3t = pd.merge(df_temp3,df_temp2[['internal_code','parent_id']],how='left',left_on='parent_id',right_on='internal_code',indicator=True)\n            # -- now merge parent_id_y to internal code of df_temp1\n            df_temp3t.drop(['_merge'],axis=1,inplace=True)\n            df_temp3t2 = pd.merge(df_temp3t,df_temp1[['internal_code','name']],how='left',left_on='parent_id_y',right_on='internal_code',indicator=True)\n            # keep only relevant columns\n            df_temp4 = df_temp3t2[['code','name_x','name_y']]\n            df_temp5 = df_temp4[['code','name_y']]\n            df_temp5.rename(columns={'code': 'product'}, inplace=True)\n            df_temp5.rename(columns={'name_y': 'name_sector_atlas'}, inplace=True)\n            # drop XXXX / services (not in product space)\n            drop_categories = ['XXXX','unspecified','travel','transport','ict','financial']\n            df_temp5 = df_temp5[ ~(df_temp5['product'].isin(drop_categories))]\n\n            # add to df_temp_plot\n            df_temp_plott = df_plot.copy()\n            df_temp_plott['product_code'] = df_temp_plott['product_code'].astype(float)\n            df_temp5['product'] = df_temp5['product'].astype(float)\n            df_temp_plot3 = pd.merge(df_temp_plott,df_temp5,how='left',left_on='product_code',right_on='product')\n            df_temp_plot3.drop_duplicates(subset='color',inplace=True)\n            df_temp_plot3 = df_temp_plot3[['color','name_sector_atlas']]\n\n            # create color dict for legend\n            color_dict = {}\n            df_temp_plot3.reset_index(inplace=True,drop=True)\n            for ind, row in df_temp_plot3.iterrows():\n                color_dict[row['name_sector_atlas']] = row['color']\n\n            \"\"\"\n            def build_legend(data):\n                # Build a legend for matplotlib plt from dict\n                legend_elements = []\n                for key in data:\n                    legend_elements.append(Line2D([0], [0], marker='o', color='w', label=key,\n                                                    markerfacecolor=data[key], markersize=10))\n                return legend_elements\n            fig,ax = plt.subplots(1)\n            #ax.add_patch(rect) # Add the patch to the Axes\n            legend_elements = build_legend(color_dict)\n            ax.legend(handles=legend_elements, loc='upper left')\n            plt.show()\n            \"\"\"\n\n    ADD_NODE_POSITIONS_ATLAS = 1\n    if ADD_NODE_POSITIONS_ATLAS == 1:\n\n        # Load position of nodes (x, y coordinates of nodes from original Atlas file)\n        import urllib.request, json\n        with urllib.request.urlopen(\"https://www.dropbox.com/s/r601tjoulq1denf/network_hs92_4digit.json?dl=1\") as url:\n            networkjs = json.loads(url.read().decode())\n\n    CREATE_NETWORKX_OBJECT_AND_PLOT = 1\n    if CREATE_NETWORKX_OBJECT_AND_PLOT == 1:\n\n        # Convert json into python list and dictionary\n        nodes = []\n        nodes_pos = {}\n        for x in networkjs['nodes']:\n            nodes.append(int(x['id']))\n            nodes_pos[int(x['id'])] = (int(x['x']),-int(x['y']/1.5))\n\n        # Define product space edge list (based on strength from the json)\n        edges = []\n        for x in networkjs['edges']:\n            if x['strength'] > 1 or 1 == 1:\n                edges.append((int(x['source']),int(x['target'])))\n        dfe = pd.DataFrame(edges)\n        dfe.rename(columns={0: 'src'}, inplace=True)\n        dfe.rename(columns={1: 'trg'}, inplace=True)\n\n        # Only select edges of nodes that are also present in product space\n        dfe2 = pd.DataFrame(np.append(dfe['src'].values,dfe['trg'].values)) # (some products may not be in there)\n        dfe2.drop_duplicates(inplace=True)\n        dfe2.rename(columns={0: 'node'}, inplace=True)\n        dfn2 = pd.merge(df_plot,dfe2,how='left',left_on=df_plot_node_col,right_on='node',indicator=True)\n\n        # Drop products from this dataframe that are not in product space\n        dfn2 = dfn2[dfn2['_merge']=='both']\n\n        # Create networkx objects in Python\n\n        # G object = products that will be plotted\n        G=nx.from_pandas_edgelist(dfn2,'product_code','product_code')\n\n        # G2 object = all nodes and edges from the original product space\n        # - Those that are not plotted will be gray in the background,\n        # - e.g. products for which there is no info\n        G2=nx.from_pandas_edgelist(dfe,'src','trg')\n\n        # Add node attributes to networkx objects\n        # - Create a 'present' variable which indicates that these products are present in product space,\n        # - as not all products in product space are present in the data to be plotted\n        # - (e.g. because we could filter only to plot products with more than >$40 million in trade)\n        df_plot['present'] = 1\n        ATTRIBUTES = ['node_size'] + ['color'] + ['present']\n        for ATTRIBUTE in ATTRIBUTES:\n            dft = df_plot[[df_plot_node_col,ATTRIBUTE]]\n            dft['count'] = 1\n            dft = dft.groupby([df_plot_node_col,ATTRIBUTE],as_index=False)['count'].sum()\n            #** drop if missing , and drop duplicates\n            dft.dropna(inplace=True)\n            dft.drop(['count'],axis=1,inplace=True)\n            dft.drop_duplicates(subset=[df_plot_node_col,ATTRIBUTE],inplace=True)\n            dft.set_index(df_plot_node_col,inplace=True)\n            dft_dict = dft[ATTRIBUTE].to_dict()\n            for i in sorted(G.nodes()):\n                try:\n                    #G.node[i][ATTRIBUTE] = dft_dict[i]\n                    G.nodes[i][ATTRIBUTE] = dft_dict[i]\n                except Exception:\n                    #G.node[i][ATTRIBUTE] = 'Missing'\n                    G.nodes[i][ATTRIBUTE] = 'Missing'\n            for i in sorted(G2.nodes()):\n                try:\n                    #G2.node[i][ATTRIBUTE] = dft_dict[i]\n                    G2.nodes[i][ATTRIBUTE] = dft_dict[i]\n                except Exception:\n                    #G2.node[i][ATTRIBUTE] = 'Missing'\n                    G2.nodes[i][ATTRIBUTE] = 'Missing'\n\n        # Cross-check that attributes have been added correctly\n        # nx.get_node_attributes(G2,df_color)\n        # nx.get_node_attributes(G,df_color)\n\n        # Create color + size lists which networkx uses for plotting\n        groups = set(nx.get_node_attributes(G2,'color').values())\n        mapping = dict(zip(sorted(groups),count()))\n        nodes = G.nodes()\n        nodes2 = G2.nodes()\n        #colorsl = [G.node[n]['color'] for n in nodes]\n        colorsl = [G.nodes[n]['color'] for n in nodes]\n        #colorsl2 = [G2.node[n]['color'] for n in nodes2]\n        colorsl2 = [G2.nodes[n]['color'] for n in nodes2]\n        SIZE_VARIABLE = 'node_size'\n        #sizesl = [G.node[n][SIZE_VARIABLE] for n in nodes]\n        sizesl = [G.nodes[n][SIZE_VARIABLE] for n in nodes]\n        # Adjust value below to increase the PLOTTED size of nodes, depending on the resolution of the final plot\n        # (e.g. if you want to zoom in into the product space and thus set a higher resolution, you may want to set this higher)\n        #sizesl2 = [G.node[n]['node_size']*350 for n in nodes]\n        sizesl2 = [G.nodes[n]['node_size']*350 for n in nodes]\n\n        # Create matplotlib object to draw the product space\n        f = plt.figure(1,figsize=(20,20))\n        ax = f.add_subplot(1,1,1)\n\n        # turn axes off\n        plt.axis('off')\n\n        # set white background\n        f.set_facecolor('white')\n\n        # draw full product space in background, transparent with small node_size\n        nx.draw_networkx(G2,nodes_pos, node_color='gray',ax=ax,node_size=10,with_labels=False,alpha=0.1)\n\n        # draw the data fed in to the product space\n        nx.draw_networkx(G,nodes_pos, node_color=colorsl,ax=ax,node_size=sizesl2,with_labels=False,alpha=1)\n\n        # save file\n        # plt.savefig(output_dir_image)\n\n        # show the plot\n        plt.show()\n\n        if show_legend == 1:\n            # show legend as well\n            def build_legend(data):\n                # Build a legend for matplotlib plt from dict\n                legend_elements = []\n                for key in data:\n                    legend_elements.append(Line2D([0], [0], marker='o', color='w', label=key,\n                                                    markerfacecolor=data[key], markersize=10))\n                return legend_elements\n            fig,ax = plt.subplots(1)\n            #ax.add_patch(rect) # Add the patch to the Axes\n            legend_elements = build_legend(color_dict)\n            ax.legend(handles=legend_elements, loc='upper left')\n            plt.show()\n\nprint('defined product space function, ready to plot')"]},{"cell_type":"markdown","metadata":{},"source":["#### Visualizing data in the product space\n\n"]},{"cell_type":"markdown","metadata":{},"source":["First we select the country we which to visualize. We&rsquo;ll search for Saudi Arabia below, using the &rsquo;audi&rsquo; string to find out the spelling of the country in the dataset, and we input that country name when defining the dataframe of the product space (&rsquo;df<sub>ps</sub>&rsquo;).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Find out what 'country_name corresponds to Saudi Arabia\nSTRING = 'audi'\ndf_rca[df_rca['country_name'].str.contains(STRING)][['country_name']].drop_duplicates()\n# result: Saudi Arabia'"]},{"cell_type":"markdown","metadata":{},"source":["##### Country, RCA, year, export value selections\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Next we define what trade properties of Saudi Arabia we want to visualize. The example below visualizes specialiation in 2005 (year=2005, RCAcpt>1) of only those products with at least 40 million in trade value.\n\nThis data preparation happens outside of the product space function so you can inspect the dataframe before plotting.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Select country\nCOUNTRY_STRING = 'Saudi Arabia'\ndf_ps = df_rca[df_rca['country_name']==COUNTRY_STRING].copy()\n\n# Cross-check\nif df_ps.shape[0] == 0:\n    print('Country string set above does not exist in data, typed correctly?')\n    STOP\n\n# Select year\ndf_ps = df_ps[df_ps['year']==2005].copy()\n\n# Select RCA > 1\ndf_ps = df_ps[df_ps['RCAcpt']>1]\n\n# Keep only relevant columns\ndf_ps = df_ps[['product_name','export_value']]\n\n# Keep only products with minimum value threshold\nexports_min_threshold = 40000000\ndf_ps = df_ps[df_ps['export_value']>exports_min_threshold]\n\n# Show resulting dataframe\ndf_ps.sample(n=5)\n\nprint('ready to plot in product space')"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Plot in the product space\ncreate_product_space(df_plot_dataframe=df_ps,\n                     df_plot_node_col='product_code',\n                     df_node_size_col='export_value',\n                     show_legend = 0)"]},{"cell_type":"markdown","metadata":{},"source":["##### Product space with legend\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Below is a legend of the product space. There&rsquo;s also a &rsquo;show legend&rsquo; option in the &rsquo;create product space&rsquo; function but this option needs to be updated.\n\n![img](https://www.dropbox.com/s/lf4lf8ktqahnovg/Selection_032.png?dl=1)\n\nTo see exactly what node represents what product, use the Atlas for now by hovering with the mouse over a node:\n\n[https://atlas.cid.harvard.edu/explore/network?country=186&year=2018&productClass=HS&product=undefined&startYear=undefined&target=Product&partner=undefined](https://atlas.cid.harvard.edu/explore/network?country=186&year=2018&productClass=HS&product=undefined&startYear=undefined&target=Product&partner=undefined)\n\n(This can also be implemented through Python by exporting to html instead of as an image, but not implemented above yet)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## -----------&#x2013;&#x2014; Break: Assignment 1 ------------------\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### What product does Ukraine export most in 1995? (excluding services such as &rsquo;transport&rsquo;, &rsquo;ict&rsquo; etc)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### What products is Ukraine specialized in in 1995 and 2005 and how much do they export of these?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Which product is most related to the product &rsquo;Stainless steel wire&rsquo;?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Plot Ukraine in the product space in 1995.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["How would you characterize Ukraine&rsquo;s position in the product space?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Plot Ukraine in the product space in 2015.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Do you notice a difference with 1995?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Plot your own country across different years in the product space. Do the results make sense? Do you notice any patterns?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["## Predicting diversification of countries: densities / density regressions\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Shreyas will cover this.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Calculating Economic Complexity / Product Complexity\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We know from the product space and density regressions how products are related to one another and how that matters for diversification of countries.\n\nThe next step is to look at which parts of the product space are most interesting to ultimately reach / diversify into. Generally complex products are located in the center of the product space, and countries with a higher economic complexity tend to have higher economic growth.\n\n![img](imgs/complex_products_in_product_space.png)\n\n![img](https://www.dropbox.com/s/a231jw76yocjkkr/complex_products_in_product_space.png?dl=1)\n\nRecall from the lecture that the economic complexity index (ECI) and product complexity index (PCI) measures are derived from an iterative method of reflections algorithm on country diversity and product ubiquity (Hidalgo Hausmann 2009), or finding the eigenvalues of a country-product matrix (Mealy et al. 2019)\n\n![img](/home/linux/Dropbox/proj/org_zhtml_projects/product-space-eci-workshop/imgs/countries_products_eci.png)\n\n![img](https://www.dropbox.com/s/dte4vwgk4tvj3rd/countries_products_eci.png?dl=1)\n\nThe original STATA package to calculate this - by Sebastian Bustos and Muhammed Yildirim - is available at:\n\n[https://github.com/cid-harvard/ecomplexity](https://github.com/cid-harvard/ecomplexity)\n\nShreyas Gadgin Matha redeveloped the package in Python, available at [https://github.com/cid-harvard/py-ecomplexity](https://github.com/cid-harvard/py-ecomplexity)\n\n(One can also directly download the PCI value for every product from the Atlas data repository - the ECI of a country is the mean of the PCI values of the products it has a comparative advantage in.)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Using the &rsquo;py-ecomplexity&rsquo; package\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Installation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["One can install it by pointing pip (package-management system in Python) to the respective library on github, using the following command:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["!pip install ecomplexity\nprint('installed py-ecomplexity')"]},{"cell_type":"markdown","metadata":{},"source":["#### Usage\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We will again use again the original trade dataset (df\\\\<sub>orig</sub>), below.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from ecomplexity import ecomplexity\nfrom ecomplexity import proximity\n\n# To use py-ecomplexity, specify the following columns\ntrade_cols = {'time':'year',\n              'loc':'country_name',\n              'prod':'product_name',\n              'val':'export_value'}\n              \n# Then run the command\nprint('calculating ecomplexity')\ndf_ec = ecomplexity(df_orig, trade_cols)\nprint('finished calculating')\n\n# Keep selected columns\ndf_ec = df_ec[['country_name',\n               'product_name',\n               'product_code',\n               'export_value',\n               'year',\n               'pci',\n               'eci']]\n\n# Show results\ndf_ec.sample(n=10)"]},{"cell_type":"markdown","metadata":{},"source":["### Complexity weighted by destination (example: Ukraine)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["You can also calculate economic complexity by destination.\n\nHidalgo and Hausmann (2009) calculate complexity of country $c$ as the average PCI of all products for which ${RCA}_{cp}>1$.\n\nBelow we define it as the weighted average PCI, where weights are given by the value of country $c$’s exports in each product. This allows us to define an ECI for separate export markets.\n\nIn a paper with Frank Neffke and Ernesto Lopez-Cordova we use this to explore opportunities for Ukraine (to connect to European value chains):\n\n[https://growthlab.cid.harvard.edu/publications/assessing-ukraines-role-european-value-chains-gravity-equation-cum-economic](https://growthlab.cid.harvard.edu/publications/assessing-ukraines-role-european-value-chains-gravity-equation-cum-economic)\n\nLet $\\mathcal{M}$ be the set of countries that together constitute an export market (say, the EU&rsquo;s Single Market). Now, the destination-market specific ECI for country $c$ is defined as:\n\n\\begin{equation} \\label{e_ECI}\nECI_{c}^{\\mathcal{M}}=\\sum \\limits_{p} \\frac{\\sum \\limits_{d \\in \\mathcal{M}} X^{d}_{op}}{\\sum \\limits_{d \\in \\mathcal{M}} X^{d}_{o}} {PCI}_{p}   \n\\end{equation}\n\nwhere $X_{op}^{d}$ represents the exports of product $p$ from exporter $o$ to importer $d$ and an omitted subscript indicates the summation over the omitted category: $X_{o}^{d}=\\sum \\limits_{p} X_{op}^{d}$.\n\nTo calculate this, we need a dataset that has country exports **per destination** for this, which is available in the Growth Lab&rsquo;s DataVerse as:\n\n&ldquo;country<sub>partner</sub><sub>hsproduct4digit</sub><sub>years</sub><sub>2000</sub><sub>2016.csv</sub>&rdquo;\n\nAs this file above is 16 gigabytes, we will load a version of it for only Ukraine&rsquo;s exports. This file has been processed outside of Google colab using the code below:\n\ndf = pd.read<sub>csv</sub>([&rsquo;country<sub>partner</sub><sub>hsproduct4digit</sub><sub>years</sub><sub>2000</sub><sub>2016.csv</sub>&rsquo;)\ndf = df[df[&rsquo;location<sub>code</sub><sub>code</sub>&rsquo;]==&rsquo;UKR&rsquo;)\ndf = df[df[&rsquo;export<sub>value</sub>&rsquo;]>0]\ndf.to<sub>csv</sub>(&rsquo;ukr<sub>exports</sub><sub>per</sub><sub>destination.csv</sub>&rsquo;,index=False)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["print(f'loading data (from dropbox)')\ndf_ukr = pd.read_csv('https://www.dropbox.com/s/megm8qzn3jcwnqz/ukr_exports_per_destination.csv?dl=1')\nprint('loaded')\n\n# show 10 random rows\ndf_ukr.sample(n=10)"]},{"cell_type":"markdown","metadata":{},"source":["Merge PCI from products in 2000 into it (from df\\\\<sub>ec</sub> created in previous section using py-ecomplexity).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# -- add leading zeroes to dataset, e.g. 303 will be 0303\ndf_ukr.loc[ (df_ukr['hs_product_code'].astype(str).str.len()==3), 'hs_product_code']= '0'+ df_ukr['hs_product_code'].astype(str)\n# -- remove leading / trailing spaces\ndf_ukr['hs_product_code'] = df_ukr['hs_product_code'].astype(str).str.strip()\n# -- keep pcis from products in 2000\ndf_pci = df_ec[df_ec['year']==2000][['product_code','pci']].drop_duplicates(subset='product_code')\ndf_ukr = pd.merge(df_ukr,df_pci[['product_code','pci']],how='left',left_on=f'hs_product_code',right_on=f'product_code',indicator=True)\n# df_ukr['_merge'].value_counts()\n# df_ukr.drop(['_merge'],axis=1,inplace=True,errors='ignore')"]},{"cell_type":"markdown","metadata":{},"source":["Now calculate ECI by destination (weighted-by-destination average PCI).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def calc_ecimc(data,origin_col,destination_col,product_col,value_col,pci_col):\n    \"\"\"\n    Calculates weighted economic complexity by destination.\n\n    Needs a year-origin-destination-product-exportvalue-pci dataset as input.\n\n    Returns:\n        pandas dataframe with ecim\n    \"\"\"\n    dft = data.copy()\n    dft['export_value_cot'] = dft.groupby([origin_col,destination_col])[value_col].transform('sum')\n    dft['pci_x_export'] = dft[pci_col] * dft[value_col]\n    dft['pci_x_export_sum'] = dft.groupby([origin_col,destination_col])['pci_x_export'].transform('sum')\n    dft['eciMc'] = dft['pci_x_export_sum']/dft['export_value_cot']\n    dft.drop_duplicates(subset=[origin_col,destination_col],inplace=True)\n    dft = dft[[origin_col,destination_col,'eciMc']]\n\n    return dft\n\ndf_ukr_ecimc = calc_ecimc(data=df_ukr,\n                          origin_col='location_code',\n                          destination_col= 'partner_code',\n                          product_col='hs_product_code',\n                          value_col='export_value',\n                          pci_col = 'pci'\n                        )\n\n# Show 10 random rows\ndf_ukr_ecimc.sample(n=20)\n# df_ukr_ecimc.sort_values(by=['eciMc'],ascending=False)[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["Map economic complexity by destination of Ukraine.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["path = geopandas.datasets.get_path('naturalearth_lowres')\nworld = geopandas.read_file(path)\n# merge complexities into it\nworld = pd.merge(world,df_ukr_ecimc[df_ukr_ecimc['eciMc']<3],how='left',left_on=f'iso_a3',right_on='partner_code',indicator=True)\nfig, ax = plt.subplots(1, 1,figsize=(15,15))\nplt.axis('off')\n# https://matplotlib.org/stable/tutorials/colors/colormapnorms.html\ncmap = cm.coolwarm\nworld.plot(column='eciMc', ax=ax, legend=True,cmap=cmap)"]},{"cell_type":"markdown","metadata":{},"source":["Highly complex products are typically destined for the Russian market.\n\nThe detoriation in relations with Russia led to a significant decline in exports there:\n![img](https://www.dropbox.com/s/xfl3gig3zxer0fm/total_exports_Ukraine_over_years.png?dl=1)\n\nAs a result, Ukraine suffers from not only a quantitative but also a qualitative decline in exports. In the paper we explore new opportunities for Ukraine.\n\nNote: double-check political controversies when using geopandas!\n\n![img](https://www.dropbox.com/s/twtl8p5ksgfezm0/map_ukraine.png?dl=1)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## -----------&#x2013;&#x2014; Break: Assignment ------------------\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### What are countries with high complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Vice versa, what are countries with low complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### What are products (PCI) with high complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Vice versa, what are products (PCI) with low complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["### Ukraine\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### How did Ukraine&rsquo;s economic complexity evolve over time?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["#### How does Ukraine&rsquo;s economic complexity in 2015 compare to other countries? Which countries have comparable economic complexity?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["#### What are the most complex products that Ukraine exported in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{},"source":["## ---\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## ---\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## ---\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Assignments answers\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Assignment 1:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### What product does Ukraine export most in 1995? (excluding services such as &rsquo;transport&rsquo;, &rsquo;ict&rsquo; etc)\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df2 = df_orig[ (df_orig['country_name']=='Ukraine') & (df_orig['year'] == 2005) ].copy()\ndf3 = df2.groupby(['product_code','product_name'],as_index=False)['export_value'].sum()\ndf3.sort_values(by=['export_value'],ascending=False,inplace=True)\ndf3[['product_name','export_value']][0:5]"]},{"cell_type":"markdown","metadata":{},"source":["#### What products is Ukraine specialized in in 1995 and 2005 and how much do they export of these?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# 1995\n\n# Use the 'df_rca' dataframe for this\n\ndf2 = df_rca[ (df_rca['year']==1995) & (df_rca['country_name']=='Ukraine')].copy()\ndf2.sort_values(by=['RCAcpt'],ascending=False,inplace=True)\ndf2[['product_name','RCAcpt','year','export_value']][0:5]\n\n# 2005\ndf2 = df_rca[ (df_rca['year']==2005) & (df_rca['country_name']=='Ukraine')].copy()\ndf2.sort_values(by=['RCAcpt'],ascending=False,inplace=True)\ndf2[['product_name','RCAcpt','year','export_value']][0:5]"]},{"cell_type":"markdown","metadata":{},"source":["#### Which product is most related to the product &rsquo;Stainless steel wire&rsquo;?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["PRODUCT = 'Stainless steel wire'\n# select only this product\ndft = df_cppt[df_cppt['product_name_1']==PRODUCT].copy()\n# Sort from high to low y Crtp\ndft.sort_values(by=['phi'],ascending=False,inplace=True)\n# Show only first row\ndft[0:1]"]},{"cell_type":"markdown","metadata":{},"source":["#### Plot Ukraine in the product space in 1995.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["How would you characterize Ukraine&rsquo;s position in the product space?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Select country\nCOUNTRY_STRING = 'Ukraine'\ndf_ps = df_rca[df_rca['country_name']==COUNTRY_STRING].copy()\n\n# Cross-check\nif df_ps.shape[0] == 0:\n    print('Country string set above does not exist in data, typed correctly?')\n    STOP\n\n# Select year\ndf_ps = df_ps[df_ps['year']==1995].copy()\n#df_ps = df_ps[df_ps['year']==2005].copy()\n\n# Select RCA > 1\ndf_ps = df_ps[df_ps['RCAcpt']>1]\n\n# Keep only relevant columns\ndf_ps = df_ps[['product_name','export_value']]\n\n# Keep only products with minimum value threshold\nexports_min_threshold = 40000000\ndf_ps = df_ps[df_ps['export_value']>exports_min_threshold]\n\n# Show resulting dataframe\ndf_ps.sample(n=5)\n\n# And finally plot in the product space\ncreate_product_space(df_plot_dataframe=df_ps,\n                     df_plot_node_col='product_code',\n                     df_node_size_col='export_value')\nprint('plotted')"]},{"cell_type":"markdown","metadata":{},"source":["#### Plot Ukraine in the product space in 2015.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Do you notice a difference with 1995?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Select country\nCOUNTRY_STRING = 'Ukraine'\ndf_ps = df_rca[df_rca['country_name']==COUNTRY_STRING].copy()\n\n# Cross-check\nif df_ps.shape[0] == 0:\n    print('Country string set above does not exist in data, typed correctly?')\n    STOP\n\n# Select year\ndf_ps = df_ps[df_ps['year']==2015].copy()\n#df_ps = df_ps[df_ps['year']==2005].copy()\n\n# Select RCA > 1\ndf_ps = df_ps[df_ps['RCAcpt']>1]\n\n# Keep only relevant columns\ndf_ps = df_ps[['product_name','export_value']]\n\n# Keep only products with minimum value threshold\nexports_min_threshold = 40000000\ndf_ps = df_ps[df_ps['export_value']>exports_min_threshold]\n\n# Show resulting dataframe\ndf_ps.sample(n=5)\n\n# And finally plot in the product space\ncreate_product_space(df_plot_dataframe=df_ps,\n                     df_plot_node_col='product_code',\n                     df_node_size_col='export_value',\n                     show_legend = 0)\nprint('plotted')"]},{"cell_type":"markdown","metadata":{},"source":["#### Plot your own country across different years in the product space. Do the results make sense? Do you notice any patterns?\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Assignment 2:\n\n"]},{"cell_type":"markdown","metadata":{},"source":["#### What are countries with high complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["qt_high = df_ec[df_ec['year']==2015]['eci'].quantile(0.95)\ndf_ec[df_ec['eci']>qt_high][['country_name']].drop_duplicates()[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### Vice versa, what are countries with low complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["qt_low = df_ec[df_ec['year']==2015]['eci'].quantile(0.05)\ndf_ec[df_ec['eci']<qt_low][['country_name']].drop_duplicates()[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### What are products (PCI) with high complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["qt_high = df_ec[df_ec['year']==2015]['pci'].quantile(0.95)\ndf_ec[df_ec['pci']>qt_high][['product_name']].drop_duplicates()[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### Vice versa, what are products (PCI) with low complexity in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["qt_low = df_ec[df_ec['year']==2015]['pci'].quantile(0.05)\ndf_ec[df_ec['pci']<qt_low][['product_name','pci']].drop_duplicates()[0:10]"]},{"cell_type":"markdown","metadata":{},"source":["#### Ukraine\n\n"]},{"cell_type":"markdown","metadata":{},"source":["##### How did Ukraine&rsquo;s economic complexity evolve over time?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df = df_ec[df_ec['country_name']=='Ukraine']\n# drop duplicates of products\ndf.drop_duplicates(subset=['country_name','year'],inplace=True)\n# keep relevant columns\ndf = df[['country_name','year','eci']]\n# sort by ECI\ndf.sort_values(by='year',ascending=False,inplace=True)\ndf.reset_index(inplace=True,drop=True)\ndf.plot(x='year', y='eci')"]},{"cell_type":"markdown","metadata":{},"source":["##### How does Ukraine&rsquo;s economic complexity in 2015 compare to other countries? Which countries have comparable economic complexity?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df = df_ec[df_ec['year']==2015].copy()\n# drop duplicates of countries\ndf = df[['country_name','eci']].drop_duplicates()\n# sort by ECI\ndf.sort_values(by='eci',ascending=False,inplace=True)\ndf.reset_index(inplace=True,drop=True)\n# create rank variable\ndf['rank'] = df.index\n# get rank of Ukraine\nRANK_UKRAINE = df[df['country_name']=='Ukraine'].reset_index()['rank'][0]\n# check countries ranked directly above and below Ukraine\ndf[ (df['rank']>RANK_UKRAINE-10) & (df['rank']<RANK_UKRAINE+10)]"]},{"cell_type":"markdown","metadata":{},"source":["##### What are the most complex products that Ukraine exported in 2015?\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["df = df_ec[df_ec['country_name']=='Ukraine'].copy()\ndf = df[df['year']==1995]\ndf.sort_values(by=['pci'],ascending=False,inplace=True)\ndf.reset_index(inplace=True,drop=True)\ndf[0:10][['product_name','pci']]"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
